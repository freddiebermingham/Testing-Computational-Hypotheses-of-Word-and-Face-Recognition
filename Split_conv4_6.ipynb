{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Split conv4-6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddiebermingham/Testing-Computational-Hypotheses-of-Word-and-Face-Recognition/blob/main/Split_conv4_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYGgOSdi3JQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7086ce52-5d10-4adc-d191-34e421243c75"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-a4S9Jo3JQN"
      },
      "source": [
        "transform = transforms.ToTensor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j7w9yGk3JQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b343581c-99aa-4d93-a7f9-47eba9d72d9e"
      },
      "source": [
        "train_data = datasets.ImageFolder(('/content/gdrive/MyDrive/WF/train'), transform=transform)\n",
        "test_data = datasets.ImageFolder(('/content/gdrive/MyDrive/WF/test'), transform=transform)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=10, shuffle=True)\n",
        "\n",
        "class_names = train_data.classes\n",
        "labels = train_data.targets\n",
        "\n",
        "print(class_names)\n",
        "print(f'Training images available: {len(train_data)}')\n",
        "print(f'Testing images available:  {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Aidan_Turner', 'Ashley_Rickards', 'Christina_Hendricks', 'Christine_Lahti', 'Cillian_Murphy', 'Dawn_French', 'Jacqueline_MacInnes_Wood', 'Kari_Matchett', 'Martha_Plimpton', 'Rowan_Atkinson', 'cash ', 'cats', 'fact', 'foot', 'heat', 'holt', 'lots', 'safe', 'soft', 'tell']\n",
            "Training images available: 9004\n",
            "Testing images available:  1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwRdzmQu3JQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76c5159-3322-412b-cc27-8f3b6d2f9902"
      },
      "source": [
        "train_data.classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Aidan_Turner',\n",
              " 'Ashley_Rickards',\n",
              " 'Christina_Hendricks',\n",
              " 'Christine_Lahti',\n",
              " 'Cillian_Murphy',\n",
              " 'Dawn_French',\n",
              " 'Jacqueline_MacInnes_Wood',\n",
              " 'Kari_Matchett',\n",
              " 'Martha_Plimpton',\n",
              " 'Rowan_Atkinson',\n",
              " 'cash ',\n",
              " 'cats',\n",
              " 'fact',\n",
              " 'foot',\n",
              " 'heat',\n",
              " 'holt',\n",
              " 'lots',\n",
              " 'safe',\n",
              " 'soft',\n",
              " 'tell']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j5e6fbh3JQS"
      },
      "source": [
        "torch.manual_seed(500)  \n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hSUTKjz3JQS"
      },
      "source": [
        "np.set_printoptions(formatter=dict(int=lambda x: f'{x:5}'))\n",
        "\n",
        "for images,labels in train_loader: \n",
        "    break\n",
        "\n",
        "print('Label:', labels.numpy())\n",
        "print('Class: ', *np.array([class_names[i] for i in labels]))\n",
        "\n",
        "\n",
        "im = make_grid(images, nrow=5) \n",
        "plt.figure(figsize=(10,4))\n",
        "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW7vfq903JQT"
      },
      "source": [
        "class ConvolutionalNetwork(nn.Sequential):\n",
        "    def __init__(self):\n",
        "        super(ConvolutionalNetwork, self).__init__()\n",
        "        out1 = 32\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 24, 3, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(24, 12, 3, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(12, 24, 3, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(24, out1, 3, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2))\n",
        "            \n",
        "            \n",
        "        self.layer_m = nn.Sequential(\n",
        "            nn.Conv2d(out1, 32, 3, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2))\n",
        "        \n",
        "        self.layer_c = nn.Sequential(\n",
        "            nn.Conv2d(out1, 32, 3, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2))\n",
        "        \n",
        "        self.fc_m = nn.Sequential(\n",
        "            nn.Linear(5*5*32, 240),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(240,20))\n",
        "        \n",
        "        self.fc_c = nn.Sequential(\n",
        "            nn.Linear(5*5*32, 240),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(240,20))\n",
        "\n",
        "    def forward(self, X):\n",
        "        X1 = self.layer1(X)\n",
        "        Xm = self.layer_m(X1)\n",
        "        Xc = self.layer_c(X1)   \n",
        "        \n",
        "        Xm = Xm.view(-1, 5*5*32)\n",
        "        Xc = Xc.view(-1, 5*5*32)\n",
        "        \n",
        "        Xm = self.fc_m(Xm)\n",
        "        Xc = self.fc_c(Xc)\n",
        "        \n",
        "        return F.log_softmax(Xm, dim=1)\n",
        "        return F.log_softmax(Xc, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwySqNLc3JQU"
      },
      "source": [
        "torch.manual_seed(500)\n",
        "model = ConvolutionalNetwork()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5g_hRMV3JQW"
      },
      "source": [
        "def count_parameters(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    for item in params:\n",
        "        print(f'{item:>6}')\n",
        "    print(f'______\\n{sum(params):>6}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuFtjRdD3JQX"
      },
      "source": [
        "count_parameters(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjitzbvD3JQY"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7Gz6SGI3JQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bd8447-6364-47a2-a46a-c3c4a03d5c3a"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "epochs = 10\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    trn_corr = 0\n",
        "    tst_corr = 0\n",
        "    \n",
        "    # training batches\n",
        "    for b, (X_train, y_train) in enumerate(train_loader):\n",
        "        b+=1\n",
        "        \n",
        "        y_pred = model(X_train)\n",
        "        loss = criterion(y_pred, y_train)\n",
        " \n",
        "        # Tally number of correct predictions\n",
        "        predicted = torch.max(y_pred.data, 1)[1]\n",
        "        batch_corr = (predicted == y_train).sum()\n",
        "        trn_corr += batch_corr\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # results\n",
        "        if b%100 == 0:\n",
        "            print(f'epoch: {i:2}  batch: {b:4} [{10*b:6}/9000]  loss: {loss.item():10.8f}  \\\n",
        "accuracy: {trn_corr.item()*100/(10*b):7.3f}%  Duration: {time.time() - start_time:.0f} seconds')\n",
        "        \n",
        "    train_losses.append(loss)\n",
        "    train_correct.append(trn_corr)\n",
        "        \n",
        "    # testing batches\n",
        "    with torch.no_grad():\n",
        "        for b, (X_test, y_test) in enumerate(test_loader):\n",
        "\n",
        "            \n",
        "            y_val = model(X_test)\n",
        "\n",
        "            # Tally number of correct predictions\n",
        "            predicted = torch.max(y_val.data, 1)[1] \n",
        "            tst_corr += (predicted == y_test).sum()\n",
        "            \n",
        "    loss = criterion(y_val, y_test)\n",
        "    test_losses.append(loss)\n",
        "    test_correct.append(tst_corr)\n",
        "        \n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0  batch:  100 [  1000/9000]  loss: 2.18429327  accuracy:  10.900%  Duration: 228 seconds\n",
            "epoch:  0  batch:  200 [  2000/9000]  loss: 1.31744087  accuracy:  26.600%  Duration: 448 seconds\n",
            "epoch:  0  batch:  300 [  3000/9000]  loss: 0.99146307  accuracy:  38.300%  Duration: 662 seconds\n",
            "epoch:  0  batch:  400 [  4000/9000]  loss: 1.08296978  accuracy:  45.425%  Duration: 862 seconds\n",
            "epoch:  0  batch:  500 [  5000/9000]  loss: 0.89173096  accuracy:  50.960%  Duration: 1051 seconds\n",
            "epoch:  0  batch:  600 [  6000/9000]  loss: 1.72418153  accuracy:  54.633%  Duration: 1293 seconds\n",
            "epoch:  0  batch:  700 [  7000/9000]  loss: 0.35340646  accuracy:  57.771%  Duration: 1464 seconds\n",
            "epoch:  0  batch:  800 [  8000/9000]  loss: 0.30336401  accuracy:  60.050%  Duration: 1634 seconds\n",
            "epoch:  0  batch:  900 [  9000/9000]  loss: 1.05529392  accuracy:  61.867%  Duration: 1803 seconds\n",
            "epoch:  1  batch:  100 [  1000/9000]  loss: 0.79795516  accuracy:  81.100%  Duration: 2083 seconds\n",
            "epoch:  1  batch:  200 [  2000/9000]  loss: 0.33279023  accuracy:  80.700%  Duration: 2102 seconds\n",
            "epoch:  1  batch:  300 [  3000/9000]  loss: 0.86564314  accuracy:  81.300%  Duration: 2121 seconds\n",
            "epoch:  1  batch:  400 [  4000/9000]  loss: 0.56356966  accuracy:  81.450%  Duration: 2140 seconds\n",
            "epoch:  1  batch:  500 [  5000/9000]  loss: 0.75100422  accuracy:  81.700%  Duration: 2159 seconds\n",
            "epoch:  1  batch:  600 [  6000/9000]  loss: 0.92677927  accuracy:  82.033%  Duration: 2178 seconds\n",
            "epoch:  1  batch:  700 [  7000/9000]  loss: 0.20492199  accuracy:  82.471%  Duration: 2198 seconds\n",
            "epoch:  1  batch:  800 [  8000/9000]  loss: 0.24795392  accuracy:  82.713%  Duration: 2217 seconds\n",
            "epoch:  1  batch:  900 [  9000/9000]  loss: 0.12370025  accuracy:  82.867%  Duration: 2236 seconds\n",
            "epoch:  2  batch:  100 [  1000/9000]  loss: 0.52667451  accuracy:  87.300%  Duration: 2273 seconds\n",
            "epoch:  2  batch:  200 [  2000/9000]  loss: 0.07981208  accuracy:  87.700%  Duration: 2293 seconds\n",
            "epoch:  2  batch:  300 [  3000/9000]  loss: 0.54141724  accuracy:  87.667%  Duration: 2312 seconds\n",
            "epoch:  2  batch:  400 [  4000/9000]  loss: 0.30031440  accuracy:  88.075%  Duration: 2331 seconds\n",
            "epoch:  2  batch:  500 [  5000/9000]  loss: 0.20073099  accuracy:  88.240%  Duration: 2351 seconds\n",
            "epoch:  2  batch:  600 [  6000/9000]  loss: 0.02152195  accuracy:  88.367%  Duration: 2370 seconds\n",
            "epoch:  2  batch:  700 [  7000/9000]  loss: 0.60184652  accuracy:  88.614%  Duration: 2390 seconds\n",
            "epoch:  2  batch:  800 [  8000/9000]  loss: 0.35995850  accuracy:  88.612%  Duration: 2409 seconds\n",
            "epoch:  2  batch:  900 [  9000/9000]  loss: 0.21606866  accuracy:  88.800%  Duration: 2428 seconds\n",
            "epoch:  3  batch:  100 [  1000/9000]  loss: 0.03590973  accuracy:  92.600%  Duration: 2464 seconds\n",
            "epoch:  3  batch:  200 [  2000/9000]  loss: 0.10688318  accuracy:  91.700%  Duration: 2484 seconds\n",
            "epoch:  3  batch:  300 [  3000/9000]  loss: 0.02838582  accuracy:  91.867%  Duration: 2503 seconds\n",
            "epoch:  3  batch:  400 [  4000/9000]  loss: 0.11412691  accuracy:  92.450%  Duration: 2523 seconds\n",
            "epoch:  3  batch:  500 [  5000/9000]  loss: 0.15808925  accuracy:  92.440%  Duration: 2542 seconds\n",
            "epoch:  3  batch:  600 [  6000/9000]  loss: 0.59784949  accuracy:  92.333%  Duration: 2562 seconds\n",
            "epoch:  3  batch:  700 [  7000/9000]  loss: 0.17817995  accuracy:  92.271%  Duration: 2581 seconds\n",
            "epoch:  3  batch:  800 [  8000/9000]  loss: 0.05216412  accuracy:  92.438%  Duration: 2601 seconds\n",
            "epoch:  3  batch:  900 [  9000/9000]  loss: 0.02820204  accuracy:  92.411%  Duration: 2621 seconds\n",
            "epoch:  4  batch:  100 [  1000/9000]  loss: 0.25505441  accuracy:  94.000%  Duration: 2657 seconds\n",
            "epoch:  4  batch:  200 [  2000/9000]  loss: 0.01419215  accuracy:  93.850%  Duration: 2676 seconds\n",
            "epoch:  4  batch:  300 [  3000/9000]  loss: 0.15726924  accuracy:  94.100%  Duration: 2696 seconds\n",
            "epoch:  4  batch:  400 [  4000/9000]  loss: 0.00845736  accuracy:  94.450%  Duration: 2715 seconds\n",
            "epoch:  4  batch:  500 [  5000/9000]  loss: 0.45976859  accuracy:  94.200%  Duration: 2734 seconds\n",
            "epoch:  4  batch:  600 [  6000/9000]  loss: 0.01159489  accuracy:  94.250%  Duration: 2752 seconds\n",
            "epoch:  4  batch:  700 [  7000/9000]  loss: 0.37666243  accuracy:  94.414%  Duration: 2771 seconds\n",
            "epoch:  4  batch:  800 [  8000/9000]  loss: 0.03163751  accuracy:  94.300%  Duration: 2790 seconds\n",
            "epoch:  4  batch:  900 [  9000/9000]  loss: 0.00226375  accuracy:  94.278%  Duration: 2809 seconds\n",
            "epoch:  5  batch:  100 [  1000/9000]  loss: 0.09198149  accuracy:  95.700%  Duration: 2844 seconds\n",
            "epoch:  5  batch:  200 [  2000/9000]  loss: 0.00120693  accuracy:  96.250%  Duration: 2863 seconds\n",
            "epoch:  5  batch:  300 [  3000/9000]  loss: 0.08218764  accuracy:  95.833%  Duration: 2882 seconds\n",
            "epoch:  5  batch:  400 [  4000/9000]  loss: 0.00074223  accuracy:  95.575%  Duration: 2901 seconds\n",
            "epoch:  5  batch:  500 [  5000/9000]  loss: 0.00791420  accuracy:  95.660%  Duration: 2920 seconds\n",
            "epoch:  5  batch:  600 [  6000/9000]  loss: 0.43420511  accuracy:  95.717%  Duration: 2939 seconds\n",
            "epoch:  5  batch:  700 [  7000/9000]  loss: 0.26390901  accuracy:  95.500%  Duration: 2958 seconds\n",
            "epoch:  5  batch:  800 [  8000/9000]  loss: 0.14641121  accuracy:  95.388%  Duration: 2977 seconds\n",
            "epoch:  5  batch:  900 [  9000/9000]  loss: 0.11469953  accuracy:  95.489%  Duration: 2997 seconds\n",
            "epoch:  6  batch:  100 [  1000/9000]  loss: 0.00363868  accuracy:  97.000%  Duration: 3032 seconds\n",
            "epoch:  6  batch:  200 [  2000/9000]  loss: 0.00405593  accuracy:  97.200%  Duration: 3051 seconds\n",
            "epoch:  6  batch:  300 [  3000/9000]  loss: 0.19225737  accuracy:  97.100%  Duration: 3070 seconds\n",
            "epoch:  6  batch:  400 [  4000/9000]  loss: 0.00576474  accuracy:  96.800%  Duration: 3089 seconds\n",
            "epoch:  6  batch:  500 [  5000/9000]  loss: 0.41085476  accuracy:  96.820%  Duration: 3108 seconds\n",
            "epoch:  6  batch:  600 [  6000/9000]  loss: 0.00810604  accuracy:  96.767%  Duration: 3127 seconds\n",
            "epoch:  6  batch:  700 [  7000/9000]  loss: 0.62475353  accuracy:  96.629%  Duration: 3146 seconds\n",
            "epoch:  6  batch:  800 [  8000/9000]  loss: 0.00003319  accuracy:  96.625%  Duration: 3165 seconds\n",
            "epoch:  6  batch:  900 [  9000/9000]  loss: 0.42770869  accuracy:  96.622%  Duration: 3184 seconds\n",
            "epoch:  7  batch:  100 [  1000/9000]  loss: 0.04926276  accuracy:  97.500%  Duration: 3220 seconds\n",
            "epoch:  7  batch:  200 [  2000/9000]  loss: 0.04038224  accuracy:  97.500%  Duration: 3238 seconds\n",
            "epoch:  7  batch:  300 [  3000/9000]  loss: 0.07894354  accuracy:  97.433%  Duration: 3258 seconds\n",
            "epoch:  7  batch:  400 [  4000/9000]  loss: 0.14399244  accuracy:  97.225%  Duration: 3277 seconds\n",
            "epoch:  7  batch:  500 [  5000/9000]  loss: 0.11311275  accuracy:  97.060%  Duration: 3296 seconds\n",
            "epoch:  7  batch:  600 [  6000/9000]  loss: 0.04053748  accuracy:  97.067%  Duration: 3315 seconds\n",
            "epoch:  7  batch:  700 [  7000/9000]  loss: 0.00300072  accuracy:  97.057%  Duration: 3335 seconds\n",
            "epoch:  7  batch:  800 [  8000/9000]  loss: 0.01245475  accuracy:  97.100%  Duration: 3354 seconds\n",
            "epoch:  7  batch:  900 [  9000/9000]  loss: 0.01350508  accuracy:  97.144%  Duration: 3373 seconds\n",
            "epoch:  8  batch:  100 [  1000/9000]  loss: 0.09798510  accuracy:  98.400%  Duration: 3408 seconds\n",
            "epoch:  8  batch:  200 [  2000/9000]  loss: 0.01368175  accuracy:  98.350%  Duration: 3427 seconds\n",
            "epoch:  8  batch:  300 [  3000/9000]  loss: 0.05465164  accuracy:  98.500%  Duration: 3447 seconds\n",
            "epoch:  8  batch:  400 [  4000/9000]  loss: 0.02041001  accuracy:  98.575%  Duration: 3465 seconds\n",
            "epoch:  8  batch:  500 [  5000/9000]  loss: 0.32548606  accuracy:  98.240%  Duration: 3484 seconds\n",
            "epoch:  8  batch:  600 [  6000/9000]  loss: 0.04917699  accuracy:  98.233%  Duration: 3503 seconds\n",
            "epoch:  8  batch:  700 [  7000/9000]  loss: 0.01528798  accuracy:  98.200%  Duration: 3522 seconds\n",
            "epoch:  8  batch:  800 [  8000/9000]  loss: 0.10530178  accuracy:  98.088%  Duration: 3541 seconds\n",
            "epoch:  8  batch:  900 [  9000/9000]  loss: 0.00103749  accuracy:  97.833%  Duration: 3559 seconds\n",
            "epoch:  9  batch:  100 [  1000/9000]  loss: 0.00562563  accuracy:  97.400%  Duration: 3594 seconds\n",
            "epoch:  9  batch:  200 [  2000/9000]  loss: 0.03955081  accuracy:  97.350%  Duration: 3613 seconds\n",
            "epoch:  9  batch:  300 [  3000/9000]  loss: 0.06762039  accuracy:  97.867%  Duration: 3632 seconds\n",
            "epoch:  9  batch:  400 [  4000/9000]  loss: 0.08764752  accuracy:  98.250%  Duration: 3651 seconds\n",
            "epoch:  9  batch:  500 [  5000/9000]  loss: 0.19199495  accuracy:  98.280%  Duration: 3670 seconds\n",
            "epoch:  9  batch:  600 [  6000/9000]  loss: 0.03187834  accuracy:  98.150%  Duration: 3689 seconds\n",
            "epoch:  9  batch:  700 [  7000/9000]  loss: 0.03602475  accuracy:  98.243%  Duration: 3708 seconds\n",
            "epoch:  9  batch:  800 [  8000/9000]  loss: 0.37824196  accuracy:  98.062%  Duration: 3726 seconds\n",
            "epoch:  9  batch:  900 [  9000/9000]  loss: 0.00467168  accuracy:  98.011%  Duration: 3745 seconds\n",
            "\n",
            "Duration: 3762 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qtdN1gy3JQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f04ded-660c-4af4-f3ec-bd039a03885b"
      },
      "source": [
        "print(test_correct)\n",
        "print()\n",
        "print(f'Test accuracy: {test_correct[-1].item()*100/1600:.3f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor(1299), tensor(1427), tensor(1492), tensor(1471), tensor(1505), tensor(1537), tensor(1516), tensor(1515), tensor(1491), tensor(1511)]\n",
            "\n",
            "Test accuracy: 94.438%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcX6CgBT3JQc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "2bb00c81-d58e-44c5-e98d-55ee706eb360"
      },
      "source": [
        "# Create a loader for the entire the test set\n",
        "test_load_all = DataLoader(test_data, batch_size=800, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    for X_test, y_test in test_load_all:\n",
        "        y_val = model(X_test)\n",
        "        predicted = torch.max(y_val,1)[1]\n",
        "        correct += (predicted == y_test).sum()\n",
        "\n",
        "arr = confusion_matrix(y_test.view(-1), predicted.view(-1))\n",
        "df_cm = pd.DataFrame(arr, class_names, class_names)\n",
        "plt.figure(figsize = (9,6))\n",
        "sn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"label (ground truth)\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1670\u001b[0m                 blocks = [\n\u001b[0;32m-> 1671\u001b[0;31m                     \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1672\u001b[0m                 ]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   2743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2744\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    130\u001b[0m             raise ValueError(\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 10, placement implies 20",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1f1adec97cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BuGn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1679\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (10, 10), indices imply (20, 20)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF01ZQYb3JQd"
      },
      "source": [
        "plt.plot([t/90 for t in train_correct], label='training accuracy')\n",
        "plt.plot([t/16 for t in test_correct], label='validation accuracy')\n",
        "plt.title('Accuracy at the end of each epoch')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BanIq8lA3JQe"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/MyDrive/SPLIT4:6.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe1wpgxQ3JQe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew7dy6MB3JQf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}